{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086660b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"C:/Users/zih028/Documents/GitHub/rl_rebalance\")\n",
    "from src.rl_agents.ppo_agent import *\n",
    "from src.eval import metrics_from_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3fd0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_minvar_targets_for_period(\n",
    "    returns: pd.DataFrame,\n",
    "    cov_dict: dict,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each date in returns, compute min-var weights from cov_dict.\n",
    "    Falls back to equal-weight if no covariance available that day.\n",
    "    \"\"\"\n",
    "    dates = returns.index\n",
    "    n_assets = returns.shape[1]\n",
    "    weights_list = []\n",
    "\n",
    "    for date in dates:\n",
    "        if date in cov_dict:\n",
    "            Sigma = cov_dict[date]\n",
    "            w = compute_minimum_variance_weights(Sigma)\n",
    "        else:\n",
    "            w = np.ones(n_assets) / n_assets\n",
    "        weights_list.append(w)\n",
    "\n",
    "    targets = pd.DataFrame(weights_list, index=dates, columns=returns.columns)\n",
    "    return targets\n",
    "\n",
    "def run_ppo_episode(\n",
    "    env: PortfolioEnvironment,\n",
    "    agent: PPOAgent,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run one full episode over env.dates with a fixed Monte Carlo (MMC) policy\n",
    "    in evaluation mode (epsilon=0, no learning).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pnl : np.ndarray\n",
    "        Daily net returns (after transaction costs)\n",
    "    tc : np.ndarray\n",
    "        Daily transaction cost as fraction of NAV\n",
    "    turnover : np.ndarray\n",
    "        Daily turnover (sum |Δw|)\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    pnls, tcs, turnovers = [], [], []\n",
    "    weights = [env.current_weights.copy()]\n",
    "\n",
    "    while True:\n",
    "        action, log_prob = agent.select_action(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # env reward = wR - TC - λ * TE^2\n",
    "        # For performance metrics we care about net return: wR - TC\n",
    "        net_ret = info[\"portfolio_return\"] - info[\"transaction_cost\"]\n",
    "\n",
    "        pnls.append(net_ret)\n",
    "        tcs.append(info[\"transaction_cost\"])\n",
    "        turnovers.append(info[\"turnover\"])\n",
    "        weights.append(env.current_weights.copy())\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    weights_df = pd.DataFrame(\n",
    "        weights[1:],\n",
    "        index = env.dates[1:],\n",
    "        columns = env.returns.columns\n",
    "    )\n",
    "\n",
    "    return np.array(pnls), np.array(tcs), np.array(turnovers), weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee5ead87",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_parquet(\"../data/returns.parquet\")\n",
    "prices = pd.read_parquet(\"../data/prices.parquet\")\n",
    "\n",
    "with open(\"../data/cov_oas_window252.pkl\", \"rb\") as f:\n",
    "    cov_dict = pickle.load(f)\n",
    "\n",
    "train_start = \"2010-01-01\"\n",
    "train_end = \"2019-12-31\"\n",
    "test_start = \"2020-01-01\"\n",
    "test_end = returns.index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "returns_train = returns.loc[train_start:train_end]\n",
    "returns_test = returns.loc[test_start:test_end]\n",
    "\n",
    "targets_train = build_minvar_targets_for_period(returns_train, cov_dict)\n",
    "targets_test = build_minvar_targets_for_period(returns_test, cov_dict)\n",
    "\n",
    "model_path = \"../model/ppo_portfolio_model.pth\"\n",
    "\n",
    "env_train = PortfolioEnvironment(\n",
    "    returns=returns_train,\n",
    "    target_weights=targets_train,\n",
    "    transaction_cost=0.001,\n",
    "    lambda_tracking=1.0,\n",
    ")\n",
    "\n",
    "env_test = PortfolioEnvironment(\n",
    "    returns=returns_test,\n",
    "    target_weights=targets_test,\n",
    "    transaction_cost=0.001,\n",
    "    lambda_tracking=1.0,\n",
    ")\n",
    "\n",
    "agent = PPOAgent(state_dim = len(env_train.reset()), action_dim = env_train.n_actions, hidden_dim=64)\n",
    "\n",
    "pnl_train, tc_train, to_train, weights_train = run_ppo_episode(env_train, agent)\n",
    "pnl_test,  tc_test,  to_test, weights_test  = run_ppo_episode(env_test, agent)\n",
    "\n",
    "metrics_train = metrics_from_pnl(pnl_train, tc_train)  # or convert tc to bps if you want\n",
    "metrics_test  = metrics_from_pnl(pnl_test,  tc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f129e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = weights_train\n",
    "df_train[\"pnl\"] = pnl_train\n",
    "df_train[\"tc\"] = tc_train\n",
    "\n",
    "df_test = weights_test\n",
    "df_test[\"pnl\"] = pnl_test\n",
    "df_test[\"tc\"] = tc_test\n",
    "\n",
    "df_train.to_csv(\"../model/ppo_train.csv\")\n",
    "df_test.to_csv(\"../model/ppo_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
